{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "zkd21a2pL8"
      },
      "source": [
        "# Compute Sample Feature Importances\n",
        "This notebook computes feature importances by dataset, treatment type, and by anomaly score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "xvb69t4oBw"
      },
      "source": [
        "import pathlib\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "sys.path.append(str((pathlib.Path.cwd().parent / \"utils\").resolve(strict=True)))\n",
        "\n",
        "import joblib\n",
        "import pandas as pd"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "KUHmTNEP3T"
      },
      "source": [
        "## Find the root of the git repo on the host system"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "vdfNeby9lU"
      },
      "source": [
        "# Get the current working directory\n",
        "cwd = pathlib.Path.cwd()\n",
        "\n",
        "if (cwd / \".git\").is_dir():\n",
        "    root_dir = cwd\n",
        "\n",
        "else:\n",
        "    root_dir = None\n",
        "    for parent in cwd.parents:\n",
        "        if (parent / \".git\").is_dir():\n",
        "            root_dir = parent\n",
        "            break\n",
        "\n",
        "# Check if a Git root directory was found\n",
        "if root_dir is None:\n",
        "    raise FileNotFoundError(\"No Git root directory found.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "f2OGzBqBg7"
      },
      "source": [
        "# Inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "FPf9gGwrq9"
      },
      "source": [
        "# Replace with your data storage path here\n",
        "big_drive_path = root_dir / \"big_drive\"\n",
        "anomaly_datasets_path = (big_drive_path / \"sc_anomaly_data\").resolve(strict=True)\n",
        "\n",
        "anomalyze_models_path = (big_drive_path / \"isolation_forest_models\").resolve(\n",
        "    strict=True\n",
        ")\n",
        "\n",
        "plate_mapping_path = (\n",
        "    root_dir / \"reference_plate_data/experiment-metadata.tsv\"\n",
        ").resolve(strict=True)\n",
        "\n",
        "plate_mappingdf = pd.read_csv(plate_mapping_path, sep=\"\\t\")[\n",
        "    [\"Assay_Plate_Barcode\", \"Perturbation\"]\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "Pd6iLYYRnQ"
      },
      "source": [
        "# Outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "VxPBfErVQM"
      },
      "source": [
        "feature_importances_path = big_drive_path / \"sc_anomaly_feature_importances\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "6QWRMmeVMb"
      },
      "source": [
        "# Sample and Compute Feature feature_importances\n",
        "Sample and then compute feature importances between the most anomalous and the least anomalous cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "Eom828CFqT"
      },
      "source": [
        "model_suffix_name = \"_isolation_forest.joblib\"\n",
        "merge_cols = [\n",
        "    \"Metadata_Plate\",\n",
        "    \"Metadata_Well\",\n",
        "    \"Metadata_Site\",\n",
        "    \"Metadata_ObjectNumber\",\n",
        "]\n",
        "\n",
        "plate_mappingdf.rename(\n",
        "    columns={\n",
        "        \"Assay_Plate_Barcode\": \"Metadata_Plate\",\n",
        "        \"Perturbation\": \"Metadata_treatment_type\",\n",
        "    },\n",
        "    inplace=True,\n",
        ")\n",
        "\n",
        "for anomaly_dataset in anomaly_datasets_path.iterdir():\n",
        "    if \"normal\" in anomaly_dataset.stem or \"qc\" in anomaly_dataset.stem:\n",
        "        continue\n",
        "\n",
        "    anomaly_model_name = anomaly_dataset.stem + model_suffix_name\n",
        "    morphology_dataset = big_drive_path / f\"{anomaly_dataset.stem}\"\n",
        "    anomalyze_model = joblib.load(anomalyze_models_path / anomaly_model_name)\n",
        "\n",
        "    anomaly_paths = list(anomaly_dataset.rglob(\"*.parquet\"))\n",
        "\n",
        "    anomdf = pd.concat(\n",
        "        [pd.read_parquet(path) for path in anomaly_paths], ignore_index=True\n",
        "    )\n",
        "\n",
        "    anomdf = pd.merge(\n",
        "        left=anomdf, right=plate_mappingdf, on=\"Metadata_Plate\", how=\"inner\"\n",
        "    )\n",
        "\n",
        "    dataset_feature_importances_path = feature_importances_path / anomaly_dataset.stem\n",
        "\n",
        "    for treatment_type_name, treatment_typedf in anomdf.groupby(\n",
        "        \"Metadata_treatment_type\"\n",
        "    ):\n",
        "        treatment_feature_importances_path = (\n",
        "            dataset_feature_importances_path / treatment_type_name\n",
        "        )\n",
        "        treatment_feature_importances_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Morphology parquet files are separated by plate in each dataset\n",
        "        for plate in treatment_typedf[\"Metadata_Plate\"].unique():\n",
        "            subprocess.run(\n",
        "                [\n",
        "                    \"run_feature_importance.sh\",\n",
        "                    str(anomaly_dataset),\n",
        "                    plate,\n",
        "                    str(morphology_dataset),\n",
        "                    str(anomalyze_models_path / anomaly_model_name),\n",
        "                    str(treatment_feature_importances_path),\n",
        "                    str(plate_mapping_path),\n",
        "                    \"nbconverted/compute_feature_importance_by_plate.py\",\n",
        "                ]\n",
        "            )"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}