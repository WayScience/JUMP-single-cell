{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "zkd21a2pL8"
      },
      "source": [
        "# Compute Sample Feature Importances\n",
        "This notebook computes feature importances by dataset, treatment type, and by anomaly score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "xvb69t4oBw"
      },
      "source": [
        "import pathlib\n",
        "import sys\n",
        "import time\n",
        "\n",
        "sys.path.append(str((pathlib.Path.cwd().parent / \"utils\").resolve(strict=True)))\n",
        "from collections import defaultdict\n",
        "\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from isolation_forest_data_feature_importance import IsoforestFeatureImportance\n",
        "from sklearn.ensemble import IsolationForest"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "KUHmTNEP3T"
      },
      "source": [
        "## Find the root of the git repo on the host system"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "vdfNeby9lU"
      },
      "source": [
        "# Get the current working directory\n",
        "cwd = pathlib.Path.cwd()\n",
        "\n",
        "if (cwd / \".git\").is_dir():\n",
        "    root_dir = cwd\n",
        "\n",
        "else:\n",
        "    root_dir = None\n",
        "    for parent in cwd.parents:\n",
        "        if (parent / \".git\").is_dir():\n",
        "            root_dir = parent\n",
        "            break\n",
        "\n",
        "# Check if a Git root directory was found\n",
        "if root_dir is None:\n",
        "    raise FileNotFoundError(\"No Git root directory found.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "f2OGzBqBg7"
      },
      "source": [
        "# Inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "FPf9gGwrq9"
      },
      "source": [
        "# Replace with your root folder path\n",
        "anomaly_datasets_path = root_dir / \"big_drive/sc_anomaly_data\"\n",
        "\n",
        "anomalyze_models_path = root_dir / \"big_drive/isolation_forest_models\"\n",
        "\n",
        "plate_mapping_path = root_dir / \"reference_plate_data/experiment-metadata.tsv\"\n",
        "\n",
        "plate_mappingdf = pd.read_csv(plate_mapping_path, sep=\"\\t\")[\n",
        "    [\"Assay_Plate_Barcode\", \"Perturbation\"]\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "Pd6iLYYRnQ"
      },
      "source": [
        "# Outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "VxPBfErVQM"
      },
      "source": [
        "feature_importances_path = root_dir / \"big_drive/sc_feature_importances\"\n",
        "feature_importances_path.mkdir(parents=True, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "6QWRMmeVMb"
      },
      "source": [
        "# Sample and Compute Feature feature_importances\n",
        "Sample and then compute feature importances between the most anomalous and the least anomalous cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "Eom828CFqT"
      },
      "source": [
        "model_suffix_name = \"_isolation_forest.joblib\"\n",
        "merge_cols = [\n",
        "    \"Metadata_Plate\",\n",
        "    \"Metadata_Well\",\n",
        "    \"Metadata_Site\",\n",
        "    \"Metadata_ObjectNumber\",\n",
        "]\n",
        "\n",
        "plate_mappingdf.rename(\n",
        "    columns={\n",
        "        \"Assay_Plate_Barcode\": \"Metadata_Plate\",\n",
        "        \"Perturbation\": \"Metadata_treatment_type\",\n",
        "    },\n",
        "    inplace=True,\n",
        ")\n",
        "\n",
        "# Number of each type of sample per dataset, treatment type, and by anomaly score limit\n",
        "num_control_samples = 500\n",
        "num_anomalous_samples = 500\n",
        "\n",
        "for anomaly_dataset in anomaly_datasets_path.iterdir():\n",
        "\n",
        "    feature_importancesdf = []\n",
        "    anomaly_model_name = anomaly_dataset.stem + model_suffix_name\n",
        "    morphology_dataset = root_dir / f\"big_drive/{anomaly_dataset.stem}\"\n",
        "    anomalyze_model = joblib.load(anomalyze_models_path / anomaly_model_name)\n",
        "    anomalyze_model.n_jobs = -1\n",
        "\n",
        "    anomaly_paths = list(anomaly_dataset.rglob(\"*.parquet\"))\n",
        "\n",
        "    anomdf = pd.concat(\n",
        "        [pd.read_parquet(path) for path in anomaly_paths], ignore_index=True\n",
        "    )\n",
        "\n",
        "    anomdf = pd.merge(\n",
        "        left=anomdf, right=plate_mappingdf, on=\"Metadata_Plate\", how=\"inner\"\n",
        "    )\n",
        "\n",
        "    for treatment_type_name, treatment_typedf in anomdf.groupby(\n",
        "        \"Metadata_treatment_type\"\n",
        "    ):\n",
        "        treatment_typedf.columns.tolist()\n",
        "\n",
        "        for control_type in [\"negcon\", \"anomalous\"]:\n",
        "\n",
        "            # Negative controls shouldn't be anomalous and vice versa\n",
        "            if control_type == \"negcon\":\n",
        "                morphologydf = treatment_typedf.loc[\n",
        "                    treatment_typedf[\"Metadata_control_type\"] == \"negcon\"\n",
        "                ].nlargest(num_control_samples, \"Result_anomaly_score\")\n",
        "\n",
        "            else:\n",
        "                morphologydf = treatment_typedf.loc[\n",
        "                    treatment_typedf[\"Metadata_control_type\"] != \"negcon\"\n",
        "                ].nsmallest(num_anomalous_samples, \"Result_anomaly_score\")\n",
        "\n",
        "            # Morphology parquet files are separated by plate in each dataset\n",
        "            for plate in morphologydf[\"Metadata_Plate\"].unique():\n",
        "\n",
        "                platedf = pd.read_parquet(\n",
        "                    list(morphology_dataset.rglob(f\"*{plate}*.parquet\"))[0]\n",
        "                )\n",
        "\n",
        "                platedf = platedf[\n",
        "                    merge_cols + anomalyze_model.feature_names_in_.tolist()\n",
        "                ]\n",
        "\n",
        "                morphologydf = pd.merge(\n",
        "                    left=platedf, right=morphologydf, on=merge_cols, how=\"inner\"\n",
        "                )\n",
        "\n",
        "                morphologydf = morphologydf[anomalyze_model.feature_names_in_.tolist()]\n",
        "\n",
        "                # Isolation forest reference:\n",
        "                # https://ieeexplore.ieee.org/document/4781136\n",
        "                feature_importancesdf.append(\n",
        "                    IsoforestFeatureImportance(\n",
        "                        _estimators=anomalyze_model.estimators_,\n",
        "                        _morphology_data=morphologydf,\n",
        "                        _num_train_samples_per_tree=anomalyze_model.max_samples_,\n",
        "                    )()\n",
        "                )\n",
        "\n",
        "                feature_importancesdf[-1] = feature_importancesdf[-1].assign(\n",
        "                    Metadata_control_type=control_type\n",
        "                )\n",
        "\n",
        "                feature_importancesdf[-1] = feature_importancesdf[-1].assign(\n",
        "                    Metadata_treatment_type=treatment_type_name\n",
        "                )\n",
        "\n",
        "    pd.concat(feature_importancesdf, axis=0).to_parquet(\n",
        "        feature_importances_path / anomaly_dataset.stem\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}